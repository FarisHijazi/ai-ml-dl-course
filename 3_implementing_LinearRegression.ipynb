{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hpgDe3nuK0MQ",
   "metadata": {},
   "source": [
    "Course: http://cs229.stanford.edu/syllabus.html\n",
    "\n",
    "pdf from <http://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf>\n",
    "https://github.com/chasinginfinity/ml-from-scratch/blob/master/02%20Linear%20Regression%20using%20Gradient%20Descent/Linear%20Regression%20using%20Gradient%20Descent.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PH5xuPAGctpu",
   "metadata": {},
   "source": [
    "![](https://github.com/chasinginfinity/ml-from-scratch/raw/24c0c0472d87f31c65cb9ad82ff0836afce924f1/02%20Linear%20Regression%20using%20Gradient%20Descent/animation1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1yMcmRoVcsno",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/farishijazi/ai-ml-dl-course/blob/master/3_implementing_LinearRegression.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QjE_z-OGK0MV",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import IPython as ipy\n",
    "\n",
    "YouTubeVideo('aircAruvnKk', width=800, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wcLqHS3fK0MY",
   "metadata": {},
   "source": [
    "## Why weight and bias?\n",
    "\n",
    "play around and see the difference between the effects of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XtkYn76QK0MY",
   "metadata": {},
   "outputs": [],
   "source": [
    "#play around with Y=MX+b\n",
    "# why have a weight and a bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CtMPnA-BK0MZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipy.display.HTML('<iframe src=\"https://www.desmos.com/calculator/os5lfdggic\" width=\"800px\" height=\"500px\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VKDS3NIkK0Mb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could just concat a bias vector (as 1s) to the input data, and extend the weight matrix, it's more efficient that way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZjG8Rz8uK0Mc",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "(assuming theta and X are both vectors. $\\theta$ is the W\n",
    "\n",
    "$$\n",
    "\\hat{y} =\\sum_{i=0}^{d} \\theta_{i} x_{i}-y \\\\\n",
    "$$\n",
    "\n",
    "as vectors:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\theta^TX \\\\\n",
    "$$\n",
    "\n",
    "\n",
    "```python\n",
    "def predict(W, X):\n",
    "    y_pred = np.dot(W.T, X)\n",
    "    return y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U5zuZaUhK0Md",
   "metadata": {},
   "source": [
    "### loss function:\n",
    "\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\cdot  \\sum(\\hat{y}^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\cdot  \\sum(\\theta^{(i)} \\cdot  X^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "```python\n",
    "def J(y_pred, Y):\n",
    "    loss = 1 / 2 * (y_pred - Y)**2\n",
    "    return loss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6DPmPJkXK0Md",
   "metadata": {},
   "source": [
    "### gradient descent\n",
    "\n",
    "Gradient descent needs the derivative\n",
    "$$\n",
    "\\theta_{j}:=\\theta_{j}-\\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta)\n",
    "$$\n",
    "\n",
    "```python\n",
    "def W_update(W_old, j_grad, alpha=0.1):\n",
    "    W_new = W_old - alpha * j_grad\n",
    "    return W_new\n",
    "```\n",
    "\n",
    "![](https://miro.medium.com/max/1024/1*G1v2WBigWmNzoMuKOYQV_g.png)\n",
    "\n",
    "---\n",
    "\n",
    "let's find the derivative of the loss function with respect to the weights\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{2} \\cdot \\sum(\\theta^{(i)} \\cdot X^{(i)} - y^{(i)})^2 $$\n",
    "\n",
    "Derivative of the prediction:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_{j}}\\hat{y} = X_j$$\n",
    "\n",
    " \n",
    "Official derivation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) &=\\frac{\\partial}{\\partial \\theta_{j}} \\frac{1}{2}\\left(\\hat{y}-y\\right)^{2} \\\\\n",
    "&=2 \\cdot \\frac{1}{2}\\left(\\hat{y}-y\\right) \\cdot \\frac{\\partial}{\\partial \\theta_{j}}\\left(\\hat{y}-y\\right) \\\\\n",
    "&=\\left(\\hat{y}-y\\right) \\cdot \\frac{\\partial}{\\partial \\theta_{j}}\\left(\\sum_{i=0}^{d} \\theta_{i} x_{i}-y\\right) \\\\\n",
    "&=\\left(\\hat{y}-y\\right) x_{j}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "```python\n",
    "def J_gradient(y_pred, Y, X):\n",
    "    j_gradients = (y_pred - Y)*X # list of all gradients for each training sample\n",
    "    j_gradient_sum = j_gradients.sum(axis=0).reshape(W.shape[0], -1) # summing all the gradients\n",
    "    return j_gradient_sum\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "<!-- \n",
    "For one training example, the gradient update is as follows:\n",
    "\n",
    "$$\n",
    "\\theta_{j}:=\\theta_{j}-\\alpha (\\theta^{(i)} * X^{(i)}_j - y^{(i)})*(X^{(i)}_j)\n",
    "$$\n",
    " -->\n",
    "\n",
    " The value of $\\alpha$ is a hyper parameter, meaning it's not a learned parameter, you just choose is as a user\n",
    "\n",
    "like a nob\n",
    "\n",
    "<img src=\"https://www.lampandlight.eu/blog/wp-content/uploads/sites/7/2019/04/Blog700x510_dimmen-1.png\" width=200px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iJuie896K0Me",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def show_plot(y_pred, Y):\n",
    "    plt.scatter(np.arange(len(Y)), Y, label='data')\n",
    "    plt.plot(y_pred, label='prediction', color='red')\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tQ4x_1-ua5wM",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW find derivative of (J with respect to b) in Y=WX+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1USMMr8vK0Mf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(W, X):\n",
    "    y_pred = np.dot(X, W)\n",
    "    return y_pred\n",
    "\n",
    "def J(y_pred, Y):\n",
    "    loss = 1 / 2 * (y_pred - Y)**2\n",
    "    return loss\n",
    "\n",
    "def J_gradient(y_pred, Y, X):\n",
    "    j_gradients = (y_pred - Y)*X # list of all gradients for each training sample\n",
    "#     return j_gradients\n",
    "    j_gradient_sum = j_gradients.sum(axis=0) # taking mean of gradients\n",
    "    j_gradient_sum = np.expand_dims(j_gradient_sum, -1) # reshaping so it will have same shape as W\n",
    "    return j_gradient_sum\n",
    "\n",
    "def W_update(W_old, j_grad, alpha=0.0001):\n",
    "    W_new = W_old - alpha * j_grad\n",
    "    return W_new\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23b7a361",
   "metadata": {},
   "source": [
    "## now implement the gradient descent algorithm\n",
    "\n",
    "all the functions are already defined\n",
    "\n",
    "<!-- hide the answer -->\n",
    "<details>\n",
    "<summary>If you REALLY give up, the answer is hiding here</summary>\n",
    "\n",
    "```python\n",
    "y_pred = predict(W, X) # make prediction\n",
    "loss = J(y_pred, Y) # calculate loss from prediction\n",
    "j_grad = J_gradient(y_pred, Y, X) # calculate gradient\n",
    "W = W_update(W, j_grad, alpha=0.0001) # update weights\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sklearn.datasets\n",
    "iris_dataset = sklearn.datasets.load_iris()\n",
    "X = iris_dataset['data']\n",
    "Y = np.expand_dims(iris_dataset['target'].T, 1)\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "## now that we chose the data, we will initialize the weights\n",
    "!rm plots/*  # delete plots for making the gif\n",
    "\n",
    "use_bias = True\n",
    "\n",
    "W = 2*np.random.uniform(size=(X.shape[1] + (1 if use_bias else 0), 1)) - 1\n",
    "if use_bias:\n",
    "    X = np.hstack((X, np.ones((X.shape[0], 1)))) ## adding bias term\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    #########################################\n",
    "    # #TODO: implement the gradient descent #\n",
    "    # hint, look at the functions above     #\n",
    "    #########################################\n",
    "    y_pred = # make prediction                ## TODO: complete this line\n",
    "    loss =   # calculate loss from prediction ## TODO: complete this line\n",
    "    j_grad = # calculate gradient             ## TODO: complete this line\n",
    "    W =      # update weights                 ## TODO: complete this line\n",
    "\n",
    "    print(f'[{i}] loss', loss.mean())\n",
    "    if i % 4 == 0:\n",
    "        print(i)\n",
    "        plt.title(f'linear regression iter:{i}. W={list(W.reshape(-1))}')\n",
    "        show_plot(y_pred, Y)\n",
    "        plt.title(f'linear regression iter:{i}. W={list(W.reshape(-1))}')\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "        plt.savefig(f'plots/{str(i).zfill(3)}.png')\n",
    "        if i % 20 == 0:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "print('y_pred', y_pred.shape)\n",
    "show_plot(y_pred, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XqHajrE1K0Mg",
   "metadata": {},
   "outputs": [],
   "source": [
    "## (optional) create GIF\n",
    "\n",
    "!apt install imagemagick > /dev/null\n",
    "!convert plots/*.png plots/linearregression1.gif\n",
    "ipy.display.HTML('<img src=\"plots/linearregression1.gif\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60lPdbvmK0Mh",
   "metadata": {},
   "source": [
    "### Comparing with `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rIsknVjeK0Mh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X, Y)\n",
    "# reg.score(X, Y)\n",
    "y_pred = reg.predict(X)\n",
    "\n",
    "show_plot(y_pred, Y)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "S0Ch1aYkK0Mi",
   "metadata": {},
   "source": [
    "SUCCESS!!\n",
    "\n",
    "We can see that our implementation and the sklearn implementation give the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bg48XhNZK0Ma",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1], [0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1], ])\n",
    "Y = np.array([[0, 1, 1, 0, 0, 1, 1, 0]]).T\n",
    "\n",
    "W = 2*np.random.uniform(size=(3, 1))-1\n",
    "b = np.zeros((1, 3))\n",
    "\n",
    "# initializing weights between -1 and 1\n",
    "plt.hist(2*np.random.uniform(size=1000)-1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "lienarregression_faris.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "83c4f5d798fd923bea524fd9771c6944d77b46017ac40633e04f6f4b632df332"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
